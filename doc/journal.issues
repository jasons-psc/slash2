06/24/2010
----------

Recently, I have removed the tiling code and instead use the pending transaction
list to hold data for log entires that need to be distilled.

Today, I have uncovered two issues with our journaling code.

(1) Right now MDS accepts updates to inode replication info and bmap CRCs
    to its private cache described as journal flush items (JFI).  They are
    flushed by the mdsfssyncthr. After flush, we close the trasaction.

    The problem is that when we close the transaction, the bmap updates
    probably have not reached the disk yet. That is controlled by ZFS.  
    mdsfssyncthr only tells ZFS that they are dirty.  The risk here is 
    that we could reclaim log space prematurely.

    We could make mdsfssyncthr do synchronous writes, but that's not good 
    for performance.

(2) Right now, we don't enforce a transaction with lower XID to be
    written earlier.  It could happen that a slot reserved by a
    trasaction is never written.   When we replay, we can find a stale
    entry in the slot.  But if we order transactions before replaying
    them.  This should be fine, assuming updates are idempotent.

    Well, life is more complicated than that. If you decide to replay XID 
    X on an item.  Then you must replay ALL transactions affecting the
    item starting from X, some of them may or may not have been overwritten.
    In other words, the history must be complete.

    The solution is easy.  Don't reserve any specific slot (remember
    traditional journal file systems reserve space amount, not specific
    location within the log area).  The idea is to consume journal space
    sequentially.  No need to reserve, just write at the current log
    tail.

(1) and (2) prompt me into thinking more on our journal code.  We already
put transaction group numbers (txn) of ZFS into our namespace log entries.  
Perhaps we should include txn into all our log entries.

If so, we can use txg to do the following:

(i) Decide whether we can reclaim log space at run time.

(ii) Decide whether we need to replay a log entry at replay time.

The possibility of unifying the way to handle both is attractive
to me.

06/25/2010
----------

Also, there will be no need to write a close log entry to mark the
closure of a transaction.  That should save us some log bandwidth.

One potential problem is that we rely on ZFS to flush its transactions
to reclaim log space.

I am thinking about invoking the log function from within ZFS, which
I can get the txg easily (like I did with the namespace).  However,
I probably have to pass in both func and data pointers so that I can
log only the changes I have made, not the entire block.


06/30/2010
----------

Here is a bit of estimate of log space consumption.  The ZFS
times out a transaction group every 30 seconds (see txg.c). Assuming
that we perform 10000 operations / second, we would need:

	30 * 10000 * 2 * 512 = 307,200,000

of log space (times 2 because each operation such as create also
needs to log bmap etc).  So this should not be a problem if we use
big enough log space.

Also, I need to call log function after dmu_tx_commit() to avoid
potential deadlock when log space is very small.  To do so, I need
to make a copy of the txg.

There used to be only one sync thread (see file mdsfssync.c) 
that commits all logs into ZFS.  Now any threads can write into ZFS 
directly.

07/07/2010
----------

Revert myself: I should do a log upcall after dmu_tx_commit() because
I want any operation to appear in our system journal first.  To
avoid deadlock with a small log file, I am going to use a reservation
system.

07/09/2010
----------

With gdb, I found out that last synced txg is not the one I recorded 
before I kill slashd on the MDS minus one.  It turns out on startup, 
ZFS does the following, which further increases txg:

(gdb) c
[Thread 0x7fffad864710 (LWP 9125) exited]

Breakpoint 1, uberblock_update (ub=0x7ffff7b9d410, rvd=0x7ffff7b9a800, txg=796) at lib/libzpool/uberblock.c:51
51              ASSERT(ub->ub_txg < txg);
(gdb) bt
#0  uberblock_update (ub=0x7ffff7b9d410, rvd=0x7ffff7b9a800, txg=796) at lib/libzpool/uberblock.c:51
#1  0x0000000000554456 in vdev_config_sync (svd=0x7fffae264dc0, svdcount=1, txg=796, tryhard=B_FALSE) at lib/libzpool/vdev_label.c:1045
#2  0x000000000054285c in spa_sync (spa=0x7ffff7b9d000, txg=796) at lib/libzpool/spa.c:4257
#3  0x00000000005490a9 in txg_sync_thread (dp=0x7fffb15e4940) at lib/libzpool/txg.c:345
#4  0x0000003be0e07761 in start_thread () from /lib64/libpthread.so.0
#5  0x0000003be0ae14dd in clone () from /lib64/libc.so.6

I need to find a way to work around this.  In addition, I need to find out
the effect of replaying ZIL on the txg (transaction group number).

Looks like ZFS loads our pool more than once, so I can't rely on spa->spa_first_txg.

07/13/2010
----------

I decided to use a ZFS special file (.sltxg) to remember the current transaction group
number.  This file is updated if we detect a txg change.  If this file is being synced
out as part of an old transaction group, new value written into the file will only appear 
in the new trasaction group.

07/15/2010
----------

ZFS does not like nested transactions - it is deadlock-prone.  So we are going to make
a thread whose job is to update the ZFS special file (now named .slcursor and contains
more fields).  We make sure that the first transaction of every group is the one created
by this thread.
