03/19/2010
----------

Looks like the hide_vnode() on a local fuse mount is failing:

root@citron: ~$ tree -a /zhihui_slash2/ | head -20
/zhihui_slash2/
|-- .slfidns
|   |-- 0
|   |   |-- 0
|   |   |   |-- 0
|   |   |   |-- 1
|   |   |   |-- 2
|   |   |   |-- 3
|   |   |   |   |-- 0004000000003ac2
|   |   |   |   |-- 0004000000003ac7
|   |   |   |   |-- 0004000000003ac8
|   |   |   |   `-- 0004000000003eb4
|   |   |   |-- 4
|   |   |   |   |-- 000400000000468d
|   |   |   |   `-- 000400000000468e
|   |   |   |       `-- newfilehere
|   |   |   |-- 5
|   |   |   |-- 6
|   |   |   |-- 7
|   |   |   |-- 8


We probably can't present both namespaces to the use at the same time to avoid confusing people/shell.

We can do some trick to convert a directory link to a regular file. But ZFS is going to read
the znode on disk and find out it is really a directory. 

Right now, I have:

root@citron: ~$ ls -ali /zhihui_slash2/.slfidns/0/0/4/
total 39
  12 drwx--x--x  6 root root  6 Mar 19 14:56 .
   7 drwx--x--x 18 root root 18 Mar 19 11:04 ..
4382 drwxr-xr-x  4 root root  3 Mar 19 14:56 000400000000468d
4383 drwxr-xr-x  3 root root  3 Mar 19 15:25 000400000000468e
   ? ?---------  ? ?    ?     ?            ? /zhihui_slash2/.slfidns/0/0/4/000400000000468b	<-- bug here
   ? ?---------  ? ?    ?     ?            ? /zhihui_slash2/.slfidns/0/0/4/000400000000468c

We can avoid traverse the two namespaces this way:

yanovich@psc.edu: we knew this when we investigated using directory hardlinking
yanovich@psc.edu: we just have to use find -maxdepth 3 on the .slfidns

03/24/2010
----------

We did an almost rewrite of the FID cache, and I did some experiments with a new test.  The setup are as follows:

root@citron: ~/projects/slash_nara/slashd$ PSC_LOG_LEVEL=3 gdb ./slashd
(gdb) r -p ~/zhihui_slash2.cf zhihui_slash2 2>/local/mds.log


root@grapefruit: ~/projects/slash_nara/mount_slash$ PSC_LOG_LEVEL=3 SLASH_MDS_NID="citron@PSC" SLASH2_PIOS_ID="bessemer@PSC" gdb ./mount_slash
(gdb) r /slash2 2> /local/client.log



zhihui@grapefruit: ~/projects/slash_nara/tests/namespace$ ./namespace -o 100000 /slash2/zhihui/testdir/

......


Files = 00011542, dirs = 000075, ops = 00011616
Files = 00011547, dirs = 000075, ops = 00011621
Files = 00011552, dirs = 000075, ops = 00011626
Files = 00011557, dirs = 000075, ops = 00011631

Delete dir operations: 0
Delete file operations: 0
Create dir operations: 74
Create file operations: 11560

Total files: 11560, dirs: 75, ops: 11634
Time used to age the directory is 1100.000000

It is very slow until I pressed control+C to stop it.

Potential reasons for slowness:

(1) Logging
	
	(gdb) shell ls -al /local/mds.log 
	-rw-r--r-- 1 root root 118159248 Mar 24 10:25 /local/mds.log
	(gdb) shell ls -al /local/client.log
	-rw-r--r-- 1 root root 360103727 Mar 24 10:26 /local/client.log

(2) test program: it has to do a lot of small allocations to scan a directory. It also
    has to do stat() to avoid reuse a filename.

(3) FID cache issues: too many duplicate asserts.

04/01/2010
----------

As of this change, new fid cache code seems to be in shape, albeit slow:

zhihui@grapefruit: ~$ ./projects/slash_nara/tests/namespace/namespace  -o 1000000 /slash2/zhihui/testdir39

....

Files = 00165256, dirs = 001925, ops = 00408488
Files = 00165257, dirs = 001925, ops = 00408493
Files = 00165258, dirs = 001925, ops = 00408498
Files = 00165262, dirs = 001926, ops = 00408503
Files = 00165259, dirs = 001926, ops = 00408508
Files = 00165260, dirs = 001926, ops = 00408513
Operation interrupted by signal 2.				<-- press control+C

Delete dir operations: 36
Delete file operations: 120628
Create dir operations: 1961
Create file operations: 285888

Total files: 165260, dirs: 1926, ops: 408513
Time used to age the directory is 19105.000000 seconds.

04/14/2010
----------

Make sure that the zp_parent field always points to the parent in the regular namespace, not the
by-id namespace.  This fixes the issue of readdir returning the right ".." information.

This fix only affects directories. We should re-create the ZFS pool to make this change take effect.

04/21/2010
----------

I have planned to let each log file to have a constant number of log entries.  That makes it easy
to find the right file that contains a log entry given its update sequence number.

We also need to support aging of updates.  If an update is, say, 30 seconds old, it will be propagated
to other MDSes immediately before the current log file is filled.

Paul proposes a idea of using batch numbers.  Basically one batch is stored in one log file.  A
log file is closed after its capacity is reached or when its old entry is too old.

Correspondingly, we can store batch numbers in the progress table for each MDS.  The problem with
this idea is that we can have lots of small files.

The other problem concerns the receiving MDS.  If it crashes in the middle of applying a log,
how can we maintain atomicity?  Should be because

	(1) We never re-use SLASH ID - used to detect if a file is already created/deleted.
	(2) ZFS guarantees internal consistency

We do have to make sure that the a file or a directory exist in both namespace (by-name
and by-id).  ZFS can only make sure one link exists or not.

We may have to stored an update sequence number in the disk inode (like zp_gen in znode_phys,
but that's used by ZFS transaction internally I guess).

Another idea is to use a different SLASH ID for each rename (possibly keep the original ZFS 
inode number).  But this does not cover the case of chmod etc.

A generation number is the best way to cope with this.  Note that within the same batch of 
changes, the same file can be changed multiple times.  But we have to add more fields into
znode_phys.

For now, we can settle down with using the timestamp fields.  They are originating from the 
same site that owns a file.

05/13/2010
----------

Stuck on how to tie our journaling with ZFS transaction.  Anyway, the following is a way
to find out the currently open transaction group in ZFS:

int
zfs_txg_info(zfsvfs_t *zfsvfs, uint64_t *txg)
{
        tx_state_t *tx; 
        tx_cpu_t *tc; 
        dsl_pool_t *dp; 

        dp = dmu_objset_pool(zfsvfs->z_os);
        tx = &dp->dp_tx;
        tc = &tx->tx_cpu[CPU_SEQID];

        mutex_enter(&tc->tc_lock);
        *txg = tx->tx_open_txg;
        mutex_exit(&tc->tc_lock);
}


