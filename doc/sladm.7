.\" $Id$
.Dd October 20, 2009
.Dt SLADM 7
.ds volume PSC - Slash Administrator's Manual
.Os http://www.psc.edu/
.Sh NAME
.Nm sladm
.Nd Slash administration guide
.Sh DESCRIPTION
Slash is a distributed network file system featuring:
.Pp
.Bl -bullet -compact -offset indent
.It
replication
.El
.Ss Creating a Slash File System
Slash uses
.Tn ZFS
for its backend metadata file system.
To create a new
.Tn ZFS
file system, launch the
.Xr zfs-fuse 8
daemon:
.Bd -literal -offset indent
# zfs-fuse
.Pp
.Ed
Next, create a
.Tn ZFS
pool with
.Xr zpool 8
which will be the master
.Tn ZFS
resource for use by the Slash file system:
.Bd -literal -offset indent
# ./zpool create mypool mirror sda sdb
.Ed
.Pp
To import the settings of this file system into Slash, a zpool cache
file is needed, which can also be created with
.Xr zpool :
.Bd -literal -offset indent
# ./zpool set cachefile=mypool.cf mypool
.Ed
.Pp
Now that the pool has been setup, the
.Xr zfs-fuse 8
daemon should be terminated so Slash be the single accessor of the new
file system:
.Bd -literal -offset indent
# pkill zfs-fuse
.Ed
.Pp
Further setup of file systems for use with Slash is necessary and can be
performed once the file system has been mounted by the metadata server.
.Ss Running Cm slashd Ns Ss ,\& the Metadata Server
.Xr slashd 8 ,
the Slash metadata server, is the first daemon that should be started
when bringing up a Slash file system.
In order to run,
.Xr slashd 8
requires an on-disk journal for resuming interrupted operations which
can be created with the
.Xr slmkjrnl 8
utility:
.Bd -literal -offset indent
# slmkjrnl
.Ed
.Pp
Next, an on-disk table for persistent storage of data pertaining to the
ownership of file blocks by
.Tn I/O
servers needs to be created with
.Xr odtable 1 :
.Bd -literal -offset indent
# odtable -C -N /var/lib/slashd/bmap_assignments.odt
.Ed
.Pp
Next, bring up
.Xr slashd 8 ,
the metadata server:
.Bd -literal -offset indent
# LNET_NETWORKS="tcp10(eth0)" LNET_ACCEPT_PORT=1202 \e
  USOCK_CPORT=1202 slashd -X -f slash.conf slpool
.Ed
.Pp
The last argument
.Dq slpool
specifies the name of the
.Tn ZFS
pool to load at startup for metadata storage.
.Pp
For normal operation, the
.Fl X
flag should
.Sy not
be specified, as this allows manipulation of internal Slash file system
structure from clients, a mode of operation that should
.Sy only
be used when initializing the metadata backing store file system via
.Xr slimmns 8 .
This utility sets up the immutable namespace and other internal
infrastructure required by the metadata server at the specified mount
point.
See
.Xr mount_slash 8
for details on bringing up a client mount point.
.Pp
After the file system has been initialized, kill
.Xr slashd 8
and relaunch it without
.Fl X :
.Bd -literal -offset indent
# LNET_NETWORKS="tcp10(eth0)" LNET_ACCEPT_PORT=1202 \e
  USOCK_CPORT=1202 slashd -f slash.conf slpool
.Ed
.Pp
Now that the metadata server is running for normal operation,
.Xr slctl 8
can be used to control its behavior.
.Ss Running Cm sliod Ns Ss ,\& the Tn Ss I/O Ss Server
The
.Tn I/O
server which manages the store of actual file data can be brought up with
.Xr sliod 8 :
.Bd -literal -offset indent
# SLASH_MDS_NID="128.182.58.86@tcp10" USOCK_CPORT=50001 \e
  LNET_NETWORKS="tcp10(eth0)" LNET_ACCEPT_PORT=1202 \e
  sliod -f slash.conf
.Ed
.Pp
.Xr sliod 8
actually be brought independently of site metadata servers.
However, on its startup,
.Xr sliod 8
tries to resume any pending operations which have not finished from previous
instances which were interrupted (for example, by machine power failure).
For this reason, it is encouraged that metadata servers be brought up before
.Tn I/O
servers.
.Pp
.Xr slioctl 8
can be used to control live operation of the
.Tn I/O
server once it is running.
.Ss Running Cm mount_slash Ns Ss ,\& the Client Mount Daemon
.Xr mount_slash 8
is used to mount a Slash file system to a point on the local system:
.Bd -literal -offset indent
# SLASH_MDS_NID="10.32.5.82@tcp10" LNET_NETWORKS="tcp10(eth0)" \e
  USOCK_CPORT=1202 SLASH2_PIOS_ID="ion@mysite" \e
  mount_slash -U -f slash.conf /myfs
.Ed
.Pp
.Xr msctl 8
can be used to control live operation of the client mount point once it
has been brought up.
.Sh FILES
.Bl -tag -width 37n -compact
.It Pa /etc/slash.conf
site configuration
.It Pa /var/run/slashd. Ns Ic %h Ns Pa .sock
metadata daemon control socket
.It Pa /var/run/sliod. Ns Ic %h Ns Pa .sock
.Tn I/O
daemon control socket
.It Pa /var/run/mount_slash. Ns Ic %h Ns Pa .sock
client mount daemon control socket
.It Pa /var/lib/slashd/bmap_assignments.odt
.Tn I/O
server file block ownership
.It Pa /var/lib/slashd/slopjrnl
pending operations journal
.El
.Sh CAVEATS
The metadata and
.Tn I/O
servers must reside on different hosts.
At this time, it is necessary to use virtual machines to facilitate an
environment where all metadata,
.Tn I/O ,
and client mount servers can run together on the same machine.
This is due to two limitations:
.Bl -bullet -offset 2n
.It
Slash uses the Lustre networking stack which restricts applications from
connecting to servers on different
.Tn TCP
ports, preventing one from running both metadata and
.Tn I/O
servers on the same machine yet bound to different
.Tn TCP
ports.
A single
.Tn TCP
port must be used site-wide for all daemons.
.It
Restrictions in the Slash protocols forbid network interface aliases
.Pq of the form Dq eth0:2
from being used either.
This is due to data being signed based on the assumption that clients
always have the same global addresses as seen by all daemons across the
site.
This condition will not hold when connections are made from different
addresses provided by the network interface aliases.
.Sh SEE ALSO
.Xr odtable 1 ,
.Xr slash.conf 5 ,
.Xr mount_slash 8 ,
.Xr msctl 8
.Xr slashd 8 ,
.Xr slctl 8 ,
.Xr slimmns 8 ,
.Xr slioctl 8 ,
.Xr sliod 8 ,
.Xr slmkjrnl 8 ,
.Xr zpool 8
